# Configuration for Llama-3-34B with Ï€-Spiral Encoding
# Phase 5: Heavyweight demo with CPU offload

experiment_name: llama_34b_pi_spiral
output_dir: ./results/llama_34b_pi_spiral
logging_dir: ./logs/llama_34b_pi_spiral

# Pretrained Model Configuration
pretrained:
  model_name_or_path: meta-llama/Meta-Llama-3-34B
  load_in_4bit: true  # 4-bit quantization required
  load_in_8bit: false
  use_flash_attn: true
  torch_dtype: bfloat16
  device_map: auto
  trust_remote_code: true
  inject_pi_spiral: true
  
  # Positional Encoding
  pos_encoding:
    type: pi_spiral
    irrational: pi
    hybrid_K: 16000
    transition_width: 1000
    max_seq_len: 128000
    rope_base: 10000.0
  
  # Attractor State
  attractor:
    use_attractor: true
    alpha_policy: exp_c_over_N
    alpha_value: 0.99
    c_value: null
    inject_layers: last_N
    N_inject: 4
    attractor_inject: cross_attn
    d_state: null

# Training Configuration
training:
  batch_size: 1
  gradient_accumulation_steps: 16
  num_epochs: 1
  max_steps: 1000
  learning_rate: 1.0e-05
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-08
  max_grad_norm: 1.0
  lr_scheduler_type: cosine
  warmup_steps: 50
  warmup_ratio: 0.05
  fp16: false
  bf16: true
  logging_steps: 5
  eval_steps: 500
  save_steps: 500
  save_total_limit: 1
  seed: 42
  deterministic: true

# Data Configuration
data:
  dataset_name: niah
  data_dir: ./data
  max_seq_length: 128000
  preprocessing_num_workers: 2
  dataloader_num_workers: 0
  dataloader_pin_memory: false
  niah_lengths:
    - 32000
    - 64000
    - 128000
  niah_depths:
    - 0.1
    - 0.5
    - 0.9
  niah_samples_per_config: 5

# Evaluation Configuration
evaluation:
  benchmarks:
    - niah
    - infinitebench
  niah_lengths:
    - 128000
  niah_depths:
    - 0.1
    - 0.3
    - 0.5
    - 0.7
    - 0.9
  ruler_max_length: 128000
  max_new_tokens: 50
  temperature: 0.1
  do_sample: false

# System Configuration
system:
  device: cuda
  num_gpus: 1
  window_size: 2000  # Small window for memory efficiency
  gradient_checkpointing: true
  cpu_offload: true  # Enable CPU offload for large model
  use_flash_attention: true
  compile_model: false
  log_memory: true
  log_throughput: true
